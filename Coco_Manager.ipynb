{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Coco Manager\n",
        "\n",
        "\n",
        "* Reads the annotations from a JSON file (i.e. `instances_train2017.json`)\n",
        "\n",
        "* Reads the images either from the disk (i.e. from `COCO/train2017/` folder),\n",
        "or by downloading them from `coco_url`s provided in annotations JSON file\n",
        "\n",
        "* Selects all of the Coco classes (81 classes total), or produces a subset to scale according to the project (i.e. if `classes = ['person']`, it will find all of the images with a person, and delete all other bounding boxes and masks from them)\n",
        "\n",
        "* Pads the images so their shapes are a multiple of some number (i.e. a multiple of 32 for a `VGG-FCN` architecture)\n",
        "\n",
        "* Shows the image alongside the semantic masks of the objects\n",
        "\n",
        "* Fits easily into a `DataLoader` (see `Usage_Coco_Manager.ipynb`)"
      ],
      "metadata": {
        "id": "8tEjnYkIBbJB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNaiRbbbwM2H"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torchvision as tv\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "from pycocotools.coco import COCO\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "import itertools\n",
        "import os\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "from skimage.morphology import disk, closing\n",
        "from skimage import io\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDqzKDFWwM40"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Coco_Manager( torch.utils.data.Dataset ):\n",
        "    \n",
        "    def __init__( self,\n",
        "                 local_imgs_path:   str=None,\n",
        "                 path_ann:          str=None,\n",
        "                 coco:              COCO=None,\n",
        "                 pad_multiplier:    int=32 ):\n",
        "        \n",
        "        self.version = 1.2\n",
        "        \n",
        "        self.local_imgs_path = local_imgs_path\n",
        "        self.coco = coco if coco else COCO( path_ann )\n",
        "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
        "        print( f'(Full Set) Images found: {len(self.ids)}' )\n",
        "\n",
        "        # Turn PIL images into Tensors\n",
        "        self.transform = tv.transforms.Compose([\n",
        "            tv.transforms.ToTensor()\n",
        "        ])\n",
        "        \n",
        "        self.pad_multiplier = pad_multiplier\n",
        "        self.possible_colors = self.create_colors()\n",
        "        np.random.shuffle(self.possible_colors)\n",
        "        \n",
        "\n",
        "    # Colors for the masks\n",
        "    def create_colors( self ) -> List[Tuple[int]]:\n",
        "\n",
        "        perm = lambda choices, n: list(itertools.product(*[choices]*n))\n",
        "        colors = perm([64*i for i in range(4)], 3) \n",
        "        return [list(rgb) for rgb in colors[1:]]\n",
        "\n",
        "\n",
        "    # Adds zero-padding to be a multiple of 2^(number of upsamplings)\n",
        "    def add_padding_multiple( self, img: torch.Tensor ) -> torch.Tensor:\n",
        "\n",
        "        mul = self.pad_multiplier\n",
        "        size = img.size()\n",
        "        pad1 = int( np.ceil( size[1] / mul ) * mul - size[1] )\n",
        "        pad2 = int( np.ceil( size[2] / mul ) * mul - size[2] )\n",
        "        imgpad = nn.functional.pad(\n",
        "            input=img,\n",
        "            pad=(pad2, 0, pad1, 0),\n",
        "            mode=\"constant\",\n",
        "            value = 0 )\n",
        "        # print( f'Added zero-padding (new size multiple of {mul}): {size} -> {imgpad.size()}' )\n",
        "        return imgpad, (pad1, pad2)\n",
        "\n",
        "\n",
        "    # If a subset of all 81 classes selected, \n",
        "    # get only images with objects from classes list.\n",
        "    # Useful for semantic segmentation.\n",
        "    def make_subset( self,\n",
        "                    classes: List,\n",
        "                    intersection: bool=False ) -> List:\n",
        "\n",
        "        catIds = self.coco.getCatIds(catNms=classes)\n",
        "\n",
        "        if intersection:\n",
        "            true_ids = self.coco.getImgIds(catIds=catIds)\n",
        "        else:\n",
        "            true_ids = []\n",
        "            for catId in catIds:\n",
        "                true_ids = true_ids + self.coco.getImgIds(catIds=[catId])\n",
        "            true_ids = list(set(true_ids))\n",
        "                \n",
        "        len_new = len(true_ids)\n",
        "        len_old = len(self.ids)\n",
        "        \n",
        "        print( f'(Subset) Images found: {len_new}/{len_old} ({round(100*len_new/len_old, 2)}%)' )\n",
        "\n",
        "        self.ids = true_ids\n",
        "        self.catIds = catIds\n",
        "        self.classes = classes\n",
        "    \n",
        "\n",
        "    # Gets an image from coco_url, if images themselves are not on disk\n",
        "    def download_image( self, id: int ) -> Tuple[np.array, List]:\n",
        "        \n",
        "        imgIds = self.coco.getImgIds(imgIds = id)\n",
        "        try:\n",
        "            img = self.coco.loadImgs(imgIds[np.random.randint(0,len(imgIds))])[0]\n",
        "            annIds = self.coco.getAnnIds( id )\n",
        "            ann = self.coco.loadAnns( annIds )\n",
        "        except:\n",
        "            print( f\"No image found with ID: {id}\" )\n",
        "            return None, None\n",
        "        for i in range(len(ann)):\n",
        "            ann[i].update({'category_name': self.coco.loadCats(ann[i]['category_id'])})\n",
        "\n",
        "        img = np.array(io.imread(img['coco_url']))\n",
        "        return img, ann\n",
        "\n",
        "\n",
        "\n",
        "    # DataLoader outputs Tuple[ image, dictionary with annotations ]\n",
        "    def collate_fn( self, data: Tuple[ torch.Tensor, Dict ]\n",
        "                  ) -> Tuple[ torch.Tensor, List[Dict] ]:\n",
        "        \n",
        "        imgs = torch.stack( [sample[0] for sample in data] )\n",
        "        anns = [sample[1] for sample in data]\n",
        "        return data\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__( self, index: int ):\n",
        "        \n",
        "        # Dataset ID to Coco ID\n",
        "        # Works in one of two modes: with local images and without\n",
        "        img_id = self.ids[index]\n",
        "        if self.local_imgs_path == None:\n",
        "            img, coco_annotations = self.download_image( img_id )\n",
        "        else:\n",
        "            ann_ids = self.coco.getAnnIds( imgIds=img_id )\n",
        "            coco_annotations = self.coco.loadAnns(ann_ids)\n",
        "            path = self.coco.loadImgs(img_id)[0]['file_name']\n",
        "            img = Image.open(os.path.join(self.local_imgs_path, path))\n",
        "\n",
        "        # Image Transformations\n",
        "        img = self.transform( img )\n",
        "        img, padding = self.add_padding_multiple( img )\n",
        "\n",
        "        # If the subset is used, erase all unnecessary annotations\n",
        "        if self.catIds:\n",
        "            coco_annotations = [obj for obj in coco_annotations if obj['category_id'] in self.catIds ]\n",
        "\n",
        "        # Object Labels\n",
        "        num_objs = len(coco_annotations)\n",
        "        for i in range(num_objs):\n",
        "            coco_annotations[i].update({'category_name': self.coco.loadCats(coco_annotations[i]['category_id'])})\n",
        "        obj_labels = [ ob['category_name'][0]['name'] for ob in coco_annotations ]\n",
        "\n",
        "        # Bounding Boxes and Masks\n",
        "        # In coco format, bbox = [xmin, ymin, width, height]\n",
        "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
        "        boxes = []\n",
        "        masks = []\n",
        "        \n",
        "        # print( f'Number of Objects: {num_objs}' )\n",
        "        for i in range(num_objs):\n",
        "            xmin = coco_annotations[i]['bbox'][0]\n",
        "            ymin = coco_annotations[i]['bbox'][1]\n",
        "            xmax = xmin + coco_annotations[i]['bbox'][2]\n",
        "            ymax = ymin + coco_annotations[i]['bbox'][3]\n",
        "            width = coco_annotations[i]['bbox'][2]\n",
        "            height = coco_annotations[i]['bbox'][3]\n",
        "            \n",
        "            # List of padded masks (so that img.size == mask.size)\n",
        "            mask = torch.tensor(self.coco.annToMask(coco_annotations[i]))\n",
        "            mask = torch.unsqueeze( mask, 0 )\n",
        "            mask, _ = self.add_padding_multiple( mask )\n",
        "            masks.append( mask )\n",
        "            boxes.append([xmin, ymin, xmax, ymax, width, height])\n",
        "    \n",
        "        # Bounding boxes aeas\n",
        "        areas = [ coco_annotations[i]['area'] for i in range(num_objs) ]\n",
        "\n",
        "        # Merge binary masks of the same class\n",
        "        # for example: 224x224x3 image with 2 classes -> torch.Size([2, 224, 224])\n",
        "        # From the greatest mask to the smallest\n",
        "        ordered_masks = np.zeros((len(self.classes), img.shape[1], img.shape[2]), dtype='uint8')\n",
        "        for i in reversed(np.argsort(np.array(areas))):\n",
        "            label = obj_labels[i]\n",
        "            mask = masks[i].numpy()[0]    \n",
        "            layer = self.classes.index( label )\n",
        "            ordered_masks[ layer, :, : ] |= mask\n",
        "            \n",
        "        # Image Annotations Dictionary\n",
        "        annotation = {}\n",
        "        annotation[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        annotation[\"masks\"] = torch.tensor(ordered_masks, dtype=torch.float32)\n",
        "        annotation[\"obj_labels\"] = obj_labels\n",
        "        annotation[\"image_id\"] = torch.tensor([img_id])\n",
        "        annotation[\"areas\"] = torch.as_tensor(areas, dtype=torch.float32)\n",
        "        annotation[\"padding\"] = torch.tensor(padding)\n",
        "        \n",
        "        return img, annotation\n",
        "    \n",
        "    def __len__( self ):\n",
        "        return len( self.ids )\n",
        "        \n",
        "\n",
        "    # Shows the image and the semantic masks\n",
        "    def show( self, img: np.array, anns: Dict ) -> None:\n",
        "        \n",
        "        with plt.style.context('dark_background'):\n",
        "        \n",
        "            fig, ax = plt.subplots(ncols=2, figsize=(15, 10), dpi=100)\n",
        "            img = np.transpose( img.numpy(), (1, 2, 0) )\n",
        "\n",
        "            bboxes = anns['boxes']\n",
        "            labels = anns['obj_labels']\n",
        "            pady, padx = anns['padding']\n",
        "            obj_num = bboxes.size()[0]\n",
        "\n",
        "            possible_colors = self.possible_colors.copy()\n",
        "            colors = {label: np.array(possible_colors.pop(0)) for label in self.classes}\n",
        "\n",
        "            # Drawing boxes on the original image\n",
        "            default_color = 'lawngreen'\n",
        "            for i in range(obj_num):\n",
        "\n",
        "                # Shifting the label color to brighter hues\n",
        "                label = labels[i]\n",
        "                color = (colors[label] / 255 + 1.5) / (1.5+1) \n",
        "\n",
        "                bbox = bboxes[i]\n",
        "                rect = mpatches.Rectangle(\n",
        "                    (padx+bbox[0], pady+bbox[1]), bbox[4], bbox[5], fill=False, edgecolor=default_color, linewidth=2)\n",
        "                circle = mpatches.Circle((padx+bbox[0]+bbox[4]//2, pady+bbox[1]+bbox[5]//2), color=default_color)\n",
        "                ax[0].add_patch( rect )\n",
        "                ax[0].add_patch( circle )\n",
        "                ax[0].text( padx+bbox[0], pady+bbox[1]-5, labels[i], fontdict={'color':default_color} )\n",
        "                ax[1].text( padx+bbox[0], pady+bbox[1]-5, labels[i], fontdict={'color':color} )\n",
        "                \n",
        "            \n",
        "            ax[0].imshow( img )\n",
        "            ax[0].set_title(f'Original Image (ID: {anns[\"image_id\"].item()})', y=1.04)\n",
        "\n",
        "            # Filling the empty canvas with masks\n",
        "            canvas = np.zeros((img.shape[0], img.shape[1], 3), dtype='uint8')\n",
        "            masks = anns['masks']\n",
        "            \n",
        "            for layer, label in enumerate(self.classes):\n",
        "                \n",
        "                color = colors[label]\n",
        "                mask = masks[layer,:,:].numpy()\n",
        "                \n",
        "                for i in range(3):\n",
        "                    canvas[:,:,i] += (mask * color[i]).astype(np.uint8) # // 3\n",
        "\n",
        "            # for i in range(3):\n",
        "            #     canvas[:,:,i] = closing(canvas[:,:,i], disk(3))\n",
        "\n",
        "            ax[1].imshow(canvas)\n",
        "            ax[1].set_title('Mask from COCO annotations', y=1.04)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Coco_Manager.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}